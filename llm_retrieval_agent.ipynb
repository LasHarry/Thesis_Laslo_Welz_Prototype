{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8dd2349-e30d-4c88-8b18-1026121b2108",
   "metadata": {},
   "source": [
    "## LLM (Phi-2) with Retrieval Agent (ColBERTv2)\n",
    "\n",
    "The collection represents the knowledge base and is loaded into the retrieval model. An indexer and searcher is initiated. \n",
    "Next, the retrieved passages are processed and a prompt is generated for each individual interaction. The interactions are\n",
    "\n",
    "1. **Compressed Prompt with limitation on the provided knowledge:** The compressed content must be used to generate and answer.\n",
    "2. **Prompt with limitation on the provided knowledge**: The content must be used to generate an answer.\n",
    "3. **Compressed Prompt without limitation on the provided knowledge:** The compressed content is presented as an inspiration to formulate an answer.\n",
    "4. **Prompt without limitation on the provided knowledge:** The content is presented as an inspiration to formulate an answer.\n",
    " \n",
    "\n",
    "The source from the external knowledge is provided for generated answers from the limited interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c79a70-c733-4e72-ae85-6713fe062844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ColBERT libraries\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "\n",
    "# Import PyTorch and HuggingFace libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llmlingua import PromptCompressor\n",
    "\n",
    "# Import default Python libraries\n",
    "import csv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd9e66-36c2-4ce6-bbaa-fd712fa35748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for available GPU. IF yes, select GPU as default device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(0)  # Set the GPU device (change the index according to your system)\n",
    "    torch.cuda.device(device)  # Set the default CUDA device\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec24ee-268a-4f4a-9e48-4aae3a13d649",
   "metadata": {},
   "source": [
    "Uncomment the next three lines to download a pre-trained checkpoint for the ColBERT indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0bfc0-9dfd-45d6-a50b-f609d906d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir -p downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd819b-c214-4add-9da5-1fcf351a5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://downloads.cs.stanford.edu/nlp/data/colbert/colbertv2/colbertv2.0.tar.gz -P downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad40f7-d9b4-4494-8595-7b6aab76b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -xvzf downloads/colbertv2.0.tar.gz -C downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb3717-709f-48c0-b8db-62d29dedea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save collection from TSV file\n",
    "collection = Collection(path='kb/collection1024token.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fedfa6-9002-4f29-9ec6-fce745475a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard parameters, checkpoint and index name. Parameters are adopted from the ColBERT documentation.\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300   # truncate passages at 300 tokens\n",
    "\n",
    "checkpoint = 'downloads/colbertv2.0'\n",
    "index_name = f'hitl.{nbits}bits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f11a2e-6c47-4746-801a-d101bcacca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample indexer fron HuggingFace or initialize a custom indexer with the pre-defined checkpoint\n",
    "APPLY_INDEXING = False\n",
    "\n",
    "if APPLY_INDEXING:\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    !mkdir \"index\"\n",
    "    indexer = snapshot_download(repo_id=\"colbert-ir/indexes\", local_dir=\"index\")\n",
    "    index_name = indexer + \"/intro_colbert\"\n",
    "else:\n",
    "    checkpoint = 'downloads/colbertv2.0'\n",
    "\n",
    "    with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
    "        config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.                                                                           # Consider larger numbers for small datasets.\n",
    "\n",
    "        indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "        indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09d0ef-7aff-4f71-b549-654a8e74696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check path to indexer file\n",
    "indexer.get_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca878af-7624-4f4b-a135-e81d017b9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ColBERT searcher\n",
    "with Run().context(RunConfig(experiment='notebook')):\n",
    "    searcher = Searcher(index=index_name, collection=collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c7537-568a-4869-9def-922a11476aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = 'How can Large Language Models be refined by external knowledge?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900014d-c31a-4f8d-9252-8c335d248428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top-3 passages for this query\n",
    "results = searcher.search(query, k=3)\n",
    "\n",
    "# Print out the top-k retrieved passages\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8230e0-0e3d-4583-a766-9617bb1ef1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = []\n",
    "metadata_list = []\n",
    "\n",
    "# extract the text and metadata from the document passage and split them into seperate lists\n",
    "for passage_id, passage_rank, passage_score in zip(*results):\n",
    "    collection_set = {searcher.collection[passage_id]}\n",
    "    for item in collection_set:\n",
    "        parts = item.split(\" metadata=\")\n",
    "        page_content_str = parts[0].split(\"page_content=\")[1].strip(\"'\")\n",
    "        content_list.append(page_content_str)\n",
    "        metadata_str = parts[1]\n",
    "        metadata_dict = ast.literal_eval(metadata_str)\n",
    "        # Only keep title, author, date and DOI for the metadata output\n",
    "        top_keys = ['title', 'author', 'publish_date', 'doi']\n",
    "        top_metadata = list(map(metadata_dict.get, top_keys))\n",
    "        metadata_list.append(top_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72578a9-30fa-415c-8b9c-9488262ddb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate metadata items\n",
    "seen = set()\n",
    "unique_meta = []\n",
    "for lst in metadata_list:\n",
    "        if lst[0] not in seen:\n",
    "            seen.add(lst[0])\n",
    "            unique_meta.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d7862-27f3-4c19-9fa7-35fec6876e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Transformer langauge model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ec1c9-1037-4d4c-a4a6-183d29bd7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define instruction text\n",
    "instruction = 'Instruction: You are an Expert on topics covering natural language processing and must answer the question:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e199e-6e04-44dc-91f4-877960b6edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare if you only want the model to use the external data\n",
    "external_knowledge_only = False\n",
    "# declare if you want to compress the prompt \n",
    "prompt_compressed = False\n",
    "\n",
    "# for interaction 1\n",
    "if external_knowledge_only and prompt_compressed:\n",
    "    llmlingua = PromptCompressor()\n",
    "    compressed_prompt = llmlingua.compress_prompt(content,\n",
    "                                                  instruction=f\"Instruction: {instruction}: {query} Below are facts that might be meaningful to answer the given question:\",\n",
    "                                                  question=\"\", target_token=300)\n",
    "    compressed_content = compressed_prompt['compressed_prompt']\n",
    "    \n",
    "    prompt = f'{instruction} {query} \\nYou confidently answer the question based on the text abstract below. If the question is not related to the text abstract, output an <missing information> message. \\n{compressed_content}'\n",
    "    metad = unique_meta\n",
    "\n",
    "# for interaction 2\n",
    "elif external_knowledge_only and not prompt_compressed:\n",
    "    prompt = f'{instruction} {query} \\nYou confidently answer the question based on the text abstract below. If the question is not related to the text abstract, output an <missing information> message. \\n{content_list}'\n",
    "    metad = unique_meta\n",
    "\n",
    "# for interaction 3\n",
    "elif not external_knowledge_only and prompt_compressed:\n",
    "    compressed_prompt = llmlingua.compress_prompt(content,\n",
    "                                                  instruction=f\"Instruction: {instruction}: {query} Below are facts that might be meaningful to answer the given question:\",\n",
    "                                                  question=\"\", target_token=300)\n",
    "    compressed_content = compressed_prompt['compressed_prompt']\n",
    "    \n",
    "    prompt = f'{instruction} {question} \\nBelow are facts that might be meaningful to answer the given question: \\n{compressed_content}'\n",
    "    # return empty metadata list\n",
    "    metad = []\n",
    "# for interaction 4\n",
    "else:\n",
    "    prompt = f'{instruction} {question} \\nBelow are facts that might be meaningful to answer the given question: \\n{content_list}'\n",
    "    # return empty metadata list\n",
    "    metad = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429f4a9-312c-4b1a-b4cc-5130da07d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output without metadata\n",
    "if not metad:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
    "    outputs = model.generate(**inputs, max_length=1000)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    print(text)\n",
    "\n",
    "# output with metadata\n",
    "else:\n",
    "    inputs = tokenizer(prompt2, return_tensors=\"pt\", return_attention_mask=False)\n",
    "    outputs = model.generate(**inputs, max_length=1000)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    print(text)\n",
    "    print('Source:')\n",
    "    for imeta in metad:\n",
    "    print(\"Title: \" + str(imeta[0]) + \"\\nAuthor: \" + str(imeta[1]) + \"\\nPublication Date: \" + str(imeta[2]) + \"\\nDOI: \" + str(imeta[3]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830635c3-c967-4eb6-99ce-2cf60a916037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
